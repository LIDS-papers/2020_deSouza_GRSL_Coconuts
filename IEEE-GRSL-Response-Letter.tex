%Fiquemos com Deus e Nossa Senhora!
\documentclass[a4paper,12pt]{article}

\usepackage[a4paper]{geometry}
\geometry{a4paper,left=20mm,right=20mm,top=35mm,bottom=35mm}


\usepackage[pdftex]{graphicx}
\usepackage[cmex10]{amsmath}
\interdisplaylinepenalty=2500
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{color}
\usepackage{multicol}
\usepackage{mathtools}

\newtheorem{theo}{Theorem}
\newtheorem{defi}{Definition}%[section]
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\usepackage{url}
\setcounter{secnumdepth}{5}

\newcommand{\rwr}[1]{\par \medskip \noindent {\bf Reviewer #1: }}
\newcommand{\ans}{\smallskip\begin{quote}  \noindent }
\newcommand{\eans}{\end{quote}}

\newcommand{\edit}{\par \medskip\noindent {\bf Editor: }}

\title{Learning CNN filters from user-drawn image markers for coconut-tree image classification}

\author{Italos Estilon de Souza and Alexandre Xavier Falc\~{a}o}

\date{Manuscript GRSL-00821-2020}

\begin{document}
\maketitle

Dear Editor, please find attached the revised version of our manuscript, GRSL-00821-2020. We thank the Editor and
reviewers for the opportunity and comments to improve the presentation of our work. We have addressed all raised issues and highlighted the changes in red. Our point-by-point response to the reviewers' comments follows next. 

\rwr{1} 
The manuscript entitled "Learning CNN ﬁlters from user-drawn image-makers for coconut-tree image classiﬁcation" presents a method that needs a minimal set of user-selected images to train the CNN’s feature extractor, reducing the number of required images to train the fully connected layers. The theme of this manuscript is interesting, and the text brings an important contribution to the remote sensing area using deep learning tools. However, to be considered adequated to publish in the journal "Geoscience and Remote Sensing Letters", it requires a revision on the text. The attached document presents all corrections and observations made for it. However, the inclusion of more experiments using more images to train the proposed model is a crucial issue in my point of view. This is important to prove the robustness of the proposed model.

\ans 

Thanks for the comments. We have improved the experimental section with new experiments, demonstrating the robustness of the proposed feature learning approach. We have also revised the text, added a figure, and clarified some parts to improve readability. All changes are in red. 

\eans


\rwr{1} A well-trained CNN model should detect image regions that best discriminate classes as positive activations, improve class separation by eliminating negative activations, and aggregate the resulting activations locally (pooling) and globally (flattening) into a highly dimensional and sparse feature space, in which a linear classifier should separate the classes with reasonable success.

\ans  Thanks. The sentence was long and unclear. We have simplified and improved it in the first paragraph of Section II.
\eans

\rwr{1} with resolution under 10 $cm^2$; LAB color space; 11387 regions; MLP classifier; 

\ans Thanks. These are minor corrections in different parts of Section III that have been solved now. We refer to the following link to a good explanation of LAB color space \url{https://en.wikipedia.org/wiki/CIELAB_color_space}. 
\eans

\rwr{1} We repeated this procedure three times to compute the mean and the standard deviation of the results in precision, recall, and f-score. 

\ans 
This sentence was removed. The dataset was randomly divided into training, validation, and testing sets. The validation set was used only once to choose empirically the architecture of the CNN through some preliminary tests. The mean results with standard deviation were computed based on those three random splits, using their respective training and testing sets. We hope this is clear in Section III now. 
\eans

\rwr{1} we identified cohesive groups of images from each class and selected two representative images from each class to draw markers.  

\ans We have changed the way to explain the choice of representative images from each class to draw markers. The new Figure 3 also helps in that explanation. \eans

\rwr{1} We have not investigated the impact of placing markers on a higher number of images and network architecture optimization procedures yet.

\ans Thanks a lot for the suggestion related to this sentence. The sentence has been removed, and we have added experiments with an increasing number of images for marker selection. The results, shown in Table II, demonstrate that FLIM is not negatively affected by choice of marker pixels in a higher number of images. On the other hand, we consider these results inconclusive. We believe it is possible to assist the user in visually identifying relevant images and regions in those images for marker selection. However, this point requires further investigation and suitable image processing with data visualization.   \eans

\rwr{1} We use Pytorch to implement our feature extractor and the MLP classifier.

\ans MLP stands for a multi-layer perceptron, and it was defined in the first paragraph of Section II. We have redefined it again in Section III. We have also added the machine used for the experiments at the beginning of the second paragraph in Section III. \eans

\rwr{1} Caption of the previous Fig. 3 (Fig. 4 now): Markers used for training in image regions with (first column) and without (second column) a coconut tree.

\ans Thanks, we have fixed that indicating which marker color is used to indicate each class. 
\eans

\rwr{1}{By comparing ``FLIM+MLP'' with VGG, one can observe that FLIM reduces the number of required training images. Finally, the comparison between ``FLIM+MLP'' and VGG-FT demonstrates that FLIM can provide competitive results using a considerably more simplified architecture than  VGG-FT.}

\ans Thanks, we have removed `` '' around the names of the methods.\eans

\rwr{2} This paper presents a new feature learning approach that can be used to reduce the number of required images to train a deep neural network. It introduces a novel way to estimate filter weights for the convolutional layers using user-drawn markers in a few training images. The need for a large number of labeled input samples is a very well known and important problem for many CNN models and the proposed solution seems to be promising but the paper has some presentation and methodological problems that must be addressed.

\ans Thanks for the comments. We have improved the experiments in Section III and the presentation of the work in several parts of the text. 
\eans


\rwr{2} Pg. 5 - Line 6: An error in the paper title is something that should be avoided at all costs as it is very confusing for the reader.  "...User-drawn Image MAKERS..." ??? The first word of the introduction is also wrong: "DePP learning"? Overall, there are not many language errors, but a proof-reading is highly recommended

\ans Thanks. We have fixed them and revised the entire text to avoid typos. \eans

\rwr{2} Pg. 9 - Lines 14-18: Justify the choice of all hyperparameters (E.g. epochs, batch size, learning rate, etc). Have any previous experiments been made to tune these hyperparameters? Has the same dataset been used?

\ans Thanks to have pointed it out. We are now explaining that the dataset is randomly divided into training (200 images), validation (2000 images), and testing (remaining images) sets. We repeated each experiment three times, each using one of these random splits. However, the validation set was used only once to choose the hyperparameters of the network. We have not developed/used any optimization technique for this purpose. We have just executed tests with a few hyperparameter choices in order to select one for the experiments. The idea was to select the simplest and yet effective network in order to demonstrate that FLIM can create competitive solutions. We are clarifying this issue (Section III) and indicating it for future work (Section IV). 
\eans

\rwr{2} Pg. 10 - Table 1: The F-score difference from FLIM+MLP (0.854) and VGG+FT (0.851) is statistically significant considering the standard deviation?  Has a statistical hypothesis test been applied?

\ans 

For three splits only and 200 training images, it was not possible to claim that this difference is statistically significant. However, they are undoubtedly competitive, being FLIM+MLP a much simpler (lighter) network than VGG-FT.   
\eans

\rwr{2} Pg. 11 - It would be worth investigating or discussing if the time taken to mark some images is always less than the time take to produce more labeled images (E.g.: how many new labeled images would take for VGG+FT beat FLIMP+MLP?). Is there a trade-off?

\ans Thanks a lot for raising both questions. We noticed that it was not clear that markers are drawn on a few images selected from the training set (we used only 4 images). The user takes a couple of minutes to select images and draw markers on those images. This time is much less than the time taken to examine and label 200 images. We added a sentence to clarify that in Section I. We also added Figure 3 to explain the user-interaction process in FLIM and modified a few parts in the text to clarify that FLIM is trained from markers in 4 images only.  

We added an experiment with increasing training set sizes. VGG-FT could never beat FLIM+MLP, even with a higher number of training images to improve its features. The reason is that 1200 images were not enough to improve VGG-FE (the feature extractor obtained at the last convolutional layer of VGG-FT). Both FLIM+MLP and VGG-FT use a similar MLP (the only difference is in the input layer due to their different number of features). Therefore, the higher number of training images benefit both approaches, being the mean f-score curve of FLIM+MLP consistently above the one of VGG-FT.

\eans

\rwr{2} Pg. 12 - It seems that the literature review outdated (only one 2019/2020 work and from the same authors).

\ans
Thanks, we have improved the literature review in Section I.
\eans

\end{document}

